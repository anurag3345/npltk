{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5muYJFBgk7Oo",
        "outputId": "fe21b205-1cfc-4b9c-fb0d-9a895bb9bae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece pandas regex tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONh8m1f22QdS",
        "outputId": "5cd0cf3c-2113-42bb-fb32-8857c4288751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/Dataset/dataset_raw.txt\"\n",
        "output_file = \"/content/data/preprocessed/corpus.txt\"\n",
        "\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "def preprocess_text_line(line):\n",
        "    # Normalize Unicode\n",
        "    line = unicodedata.normalize(\"NFC\", line)\n",
        "\n",
        "    # Skip extremely long lines to avoid regex hangs (optional)\n",
        "    if len(line) > 10000:\n",
        "        print(f\"Skipping very long line ({len(line)} chars)\")\n",
        "        return \"\"\n",
        "\n",
        "    # Remove headers exactly matching \"शीर्षक\" or \"विवरण\"\n",
        "    if re.match(r\"^\\s*(शीर्षक|विवरण)\\s*$\", line):\n",
        "        return \"\"\n",
        "\n",
        "    # Replace smart quotes with normal ones\n",
        "    line = line.replace('“', '\"').replace('”', '\"').replace('‘', \"'\").replace('’', \"'\")\n",
        "\n",
        "    # Collapse multiple spaces/tabs into one space\n",
        "    # This should be very fast on normal lines\n",
        "    line = re.sub(r\"[ \\t]+\", \" \", line)\n",
        "\n",
        "    return line.strip()\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile, \\\n",
        "     open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "\n",
        "    blank_line_written = False\n",
        "\n",
        "    for i, line in enumerate(infile, 1):\n",
        "        try:\n",
        "            processed_line = preprocess_text_line(line)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line {i}: {e}\")\n",
        "            processed_line = \"\"\n",
        "\n",
        "        if processed_line == \"\":\n",
        "            if not blank_line_written:\n",
        "                outfile.write(\"\\n\")\n",
        "                blank_line_written = True\n",
        "        else:\n",
        "            outfile.write(processed_line + \"\\n\")\n",
        "            blank_line_written = False\n",
        "\n",
        "print(f\"Preprocessed corpus saved at: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPRPL46v2keB",
        "outputId": "7a4f64b5-297e-46ff-ee44-66fdc0f1530e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping very long line (22356 chars)\n",
            "Skipping very long line (22356 chars)\n",
            "Skipping very long line (22356 chars)\n",
            "Skipping very long line (22483 chars)\n",
            "Skipping very long line (22356 chars)\n",
            "Skipping very long line (15729 chars)\n",
            "Skipping very long line (15742 chars)\n",
            "Skipping very long line (15825 chars)\n",
            "Skipping very long line (15737 chars)\n",
            "Skipping very long line (15737 chars)\n",
            "Skipping very long line (15729 chars)\n",
            "Skipping very long line (15729 chars)\n",
            "Skipping very long line (15728 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (11821 chars)\n",
            "Skipping very long line (10252 chars)\n",
            "Skipping very long line (10282 chars)\n",
            "Skipping very long line (11829 chars)\n",
            "Skipping very long line (11898 chars)\n",
            "Skipping very long line (11818 chars)\n",
            "Skipping very long line (11889 chars)\n",
            "Skipping very long line (11860 chars)\n",
            "Skipping very long line (11839 chars)\n",
            "Skipping very long line (10205 chars)\n",
            "Skipping very long line (10234 chars)\n",
            "Skipping very long line (10248 chars)\n",
            "Skipping very long line (13998 chars)\n",
            "Skipping very long line (14922 chars)\n",
            "Skipping very long line (16832 chars)\n",
            "Preprocessed corpus saved at: /content/data/preprocessed/corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Define the path to the folder containing the preprocessed file\n",
        "data_pre = Path(\"/content/data/preprocessed\")\n",
        "\n",
        "# Search for the file named \"corpus.txt\" in that folder\n",
        "pre_files = list(data_pre.glob(\"corpus.txt\"))\n",
        "\n",
        "if not pre_files:\n",
        "    raise SystemExit(\"No preprocessed files found. Run upload + preprocess steps.\")\n",
        "\n",
        "# Pick the first found file (should be only one)\n",
        "pf = pre_files[0]\n",
        "\n",
        "# Read the file contents as lines\n",
        "lines = pf.read_text(encoding='utf-8').splitlines()\n",
        "\n",
        "print(\"Preprocessed file:\", pf)\n",
        "print(\"Number of lines:\", len(lines))\n",
        "print(\"\\nFirst 10 lines:\")\n",
        "for i, l in enumerate(lines[:10], 1):\n",
        "    print(i, \"-\", l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSXMNLvi6gEn",
        "outputId": "94b88d91-73c6-41fe-ab55-e967062a694e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed file: /content/data/preprocessed/corpus.txt\n",
            "Number of lines: 5587515\n",
            "\n",
            "First 10 lines:\n",
            "1 - सुर्खेत (रासस ।\n",
            "2 - विसं २०४६ मा तत्कालीन मसालको विद्यार्थी संगठनको सदस्यता लिँदा उनी राप्ती ज्ञानोदय मावि रुकुममा कक्षा ६ मा अध्ययनरत थिइन् ।\n",
            "3 - विद्यार्थी संगठनको सदस्य हुँदा राजनीतिबारे खासै थाहा थिएन ।\n",
            "4 - पछि संगठनका अग्रजले मुलुकको राजनीतिक आर्थिक र सामाजिक अवस्थाको यथार्थता प्रष्ट्याउँदै जाँदा त्यसप्रति उनको चासो बढ्यो ।\n",
            "5 - यद्यपि त्यतिबेला भने नेपालमा सशस्त्र युद्धको थालनी भइसकेको थिएन ।\n",
            "6 - किशोर अवस्थादेखि राजनीतिक यात्रा शुरु गरेकी बिमला केसी रुकुम पश्चिमको सानीभेरी गाउँपालिका ९ सिम्ली निवासी हुन् ।\n",
            "7 - सानै उमेरमा बुबाको मृत्यु भएकाले केसीलाई विद्यालय शिक्षा हाँसिल गर्न निकै समस्या भयो ।\n",
            "8 - बुबाको मृत्युपछि आमाले भोगेको पीडा व्याप्त गरिबी र महिला हिंसाले उनलाई संघर्षमा उत्रन उत्प्रेरित गरायो ।\n",
            "9 - विद्यार्थी राजनीतिबाट अघि बढेकी केसीपछि २०५२ सालमा तत्कालीन नेकपा (माओवादी)ले सञ्चालन गरेको जनयुद्धमा सहभागी भइन् ।\n",
            "10 - पूर्णकालीन सदस्य बनेर माओवादी राजनीतिको यात्राका दौडानमा उनी रुकुम जिल्लाको पहिलो महिला विद्यार्थी तथा कलाकार संगठनको एकै पटक प्रमुख बनिन् ।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "\n",
        "subset_file = \"subset_corpus.txt\"\n",
        "\n",
        "# Define the path to the preprocessed file using the output_file variable from the previous cell\n",
        "pf = Path(output_file)\n",
        "\n",
        "# Create a subset of first 3 million lines for training\n",
        "with open(subset_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    with open(str(pf), \"r\", encoding=\"utf-8\") as f_in:\n",
        "        for i, line in enumerate(f_in):\n",
        "            if i >= 3000000:\n",
        "                break\n",
        "            f_out.write(line)\n",
        "\n",
        "model_prefix = \"nepali_tokenizer\"\n",
        "VOCAB_SIZE = 64000\n",
        "MODEL_TYPE = \"unigram\"\n",
        "CHAR_COVERAGE = 1.0\n",
        "\n",
        "spm_cmd = (\n",
        "    f\"--input={subset_file} \"\n",
        "    f\"--model_prefix={model_prefix} \"\n",
        "    f\"--vocab_size={VOCAB_SIZE} \"\n",
        "    f\"--model_type={MODEL_TYPE} \"\n",
        "    f\"--character_coverage={CHAR_COVERAGE} \"\n",
        "    f\"--shuffle_input_sentence=true \"\n",
        "    f\"--input_sentence_size=1000000\"\n",
        ")\n",
        "\n",
        "print(\"SentencePiece command:\")\n",
        "print(spm_cmd)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(spm_cmd)\n",
        "\n",
        "print(\"\\nTraining finished. Model files created:\")\n",
        "print(model_prefix + \".model\")\n",
        "print(model_prefix + \".vocab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwjTeqewHm3J",
        "outputId": "3637cf64-c03a-4727-931c-4a1d7ef34794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece command:\n",
            "--input=subset_corpus.txt --model_prefix=nepali_tokenizer --vocab_size=64000 --model_type=unigram --character_coverage=1.0 --shuffle_input_sentence=true --input_sentence_size=1000000\n",
            "\n",
            "Training finished. Model files created:\n",
            "nepali_tokenizer.model\n",
            "nepali_tokenizer.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"nepali_tokenizer.model\")\n",
        "\n",
        "def sp_pieces(text):\n",
        "    return sp.encode(text, out_type=str)\n",
        "\n",
        "def sp_decode(pieces):\n",
        "    return sp.decode_pieces(pieces)\n",
        "\n",
        "tests = [\n",
        "    \"नेपाल एक सुन्दर देश हो।\",\n",
        "    \"काठमाडौं नेपालको राजधानी हो।\",\n",
        "    \"घरहरूमा धेरै मान्छे बस्छन्।\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    pieces = sp_pieces(t)\n",
        "    recon = sp_decode(pieces)\n",
        "    print(\"\\nSENT:\", t)\n",
        "    print(\"PIECES:\", pieces)\n",
        "    print(\"RECON:\", recon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rhIUe7EMGJF",
        "outputId": "0ba30871-e7c1-476d-d070-77bae3806db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SENT: नेपाल एक सुन्दर देश हो।\n",
            "PIECES: ['▁नेपाल', '▁एक', '▁सुन्दर', '▁देश', '▁हो', '।']\n",
            "RECON: नेपाल एक सुन्दर देश हो।\n",
            "\n",
            "SENT: काठमाडौं नेपालको राजधानी हो।\n",
            "PIECES: ['▁काठमाडौं', '▁नेपालको', '▁राजधानी', '▁हो', '।']\n",
            "RECON: काठमाडौं नेपालको राजधानी हो।\n",
            "\n",
            "SENT: घरहरूमा धेरै मान्छे बस्छन्।\n",
            "PIECES: ['▁घर', 'हरूमा', '▁धेरै', '▁मान्छे', '▁बस्छन्', '।']\n",
            "RECON: घरहरूमा धेरै मान्छे बस्छन्।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "vocab_path = Path(\"nepali_tokenizer.vocab\")\n",
        "if not vocab_path.exists():\n",
        "    raise SystemExit(\"Vocab file not found. Run training cell.\")\n",
        "\n",
        "vocab_lines = vocab_path.read_text(encoding='utf-8').splitlines()\n",
        "rows = [l.split(\"\\t\") for l in vocab_lines]\n",
        "tokens = [r[0] for r in rows]\n",
        "scores = [float(r[1]) if len(r) > 1 and r[1] != \"\" else None for r in rows]\n",
        "\n",
        "df = pd.DataFrame({\"token\": tokens, \"score\": scores})\n",
        "df[\"length_chars\"] = df[\"token\"].str.len()\n",
        "\n",
        "# Display first 5 rows\n",
        "print(\"\\nFirst 5 vocab entries:\")\n",
        "print(df.head(5))\n",
        "\n",
        "# Print total unique vocab count\n",
        "unique_vocab_count = df[\"token\"].nunique()\n",
        "print(f\"\\nTotal unique tokens in vocab: {unique_vocab_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhTQdnoiRVeu",
        "outputId": "e5033467-3940-4872-f026-9c3d5fb05b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 vocab entries:\n",
            "   token    score  length_chars\n",
            "0  <unk>  0.00000             5\n",
            "1    <s>  0.00000             3\n",
            "2   </s>  0.00000             4\n",
            "3      । -2.98211             1\n",
            "4      ▁ -3.08217             1\n",
            "\n",
            "Total unique tokens in vocab: 64000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Use the preprocessed file (pf)\n",
        "ws_counts = []\n",
        "sp_counts = []\n",
        "with open(pf, 'r', encoding='utf-8') as f:\n",
        "    for ln in f:\n",
        "        ln = ln.strip()\n",
        "        if not ln:\n",
        "            continue\n",
        "        ws = ln.split()\n",
        "        # join back into a \"sentence\" (tokens already spaced), evaluate SP tokenization\n",
        "        reconstructed = \" \".join(ws)\n",
        "        ws_counts.append(len(ws))\n",
        "        sp_counts.append(len(sp.encode(reconstructed, out_type=str)))\n",
        "\n",
        "print(\"Processed lines:\", len(ws_counts))\n",
        "print(\"Average whitespace tokens per line:\", mean(ws_counts))\n",
        "print(\"Average SentencePiece tokens per line:\", mean(sp_counts))\n",
        "print(\"Average token inflation ratio (SP / whitespace):\", mean([s/w if w>0 else 0 for s,w in zip(sp_counts, ws_counts)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fXp7r85RYvV",
        "outputId": "4a48a34f-a0fd-42b0-9222-5f4e34101b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed lines: 5586977\n",
            "Average whitespace tokens per line: 16.17308107765613\n",
            "Average SentencePiece tokens per line: 19.792281407279823\n",
            "Average token inflation ratio (SP / whitespace): 1.2442565697033334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "split_words = Counter()\n",
        "with open(pf, 'r', encoding='utf-8') as f:\n",
        "    for ln in f:\n",
        "        for word in ln.strip().split():\n",
        "            sp_tokens = sp.encode(word, out_type=str)\n",
        "            if len(sp_tokens) > 1:\n",
        "                split_words[word] += 1\n",
        "\n",
        "print(\"\\nTop 20 most split words:\")\n",
        "for w, c in split_words.most_common(20):\n",
        "    print(w, \"→ split\", c, \"times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1VzN3XGLbIJ",
        "outputId": "fe84ddee-3a5c-4ef2-fcc3-32c0b0678912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20 most split words:\n",
            "। → split 4025073 times\n",
            "छ। → split 442643 times\n",
            "छन्। → split 179319 times\n",
            "को → split 126451 times\n",
            "हो। → split 97198 times\n",
            "थियो। → split 94543 times\n",
            "बताए। → split 83565 times\n",
            "का → split 69085 times\n",
            "थिए। → split 69015 times\n",
            "हुन्। → split 40887 times\n",
            "भने, → split 40493 times\n",
            "दिए। → split 34137 times\n",
            "ले → split 31513 times\n",
            "बैंकले → split 29411 times\n",
            "छैन। → split 29224 times\n",
            "गरे। → split 24039 times\n",
            "छ, → split 18298 times\n",
            "बाट → split 18211 times\n",
            "उनीहरुले → split 18200 times\n",
            "हुन्छ। → split 17331 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unk_lines = 0\n",
        "with open(pf, 'r', encoding='utf-8') as f:\n",
        "    for ln in f:\n",
        "        if \"<unk>\" in sp.encode(ln.strip(), out_type=str):\n",
        "            unk_lines += 1\n",
        "\n",
        "print(f\"\\nLines containing <unk>: {unk_lines} ({unk_lines/len(ws_counts)*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7r902CGLrFp",
        "outputId": "7c5af22f-e580-4355-f0a9-972aa1aa55cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lines containing <unk>: 0 (0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copy2\n",
        "drive_target = Path(\"/content/drive/MyDrive/nepali_tokenizer_models\")\n",
        "drive_target.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for fn in [\"nepali_tokenizer.model\", \"nepali_tokenizer.vocab\"]:\n",
        "    src = Path(fn)\n",
        "    if src.exists():\n",
        "        dst = drive_target / fn\n",
        "        copy2(src, dst)\n",
        "        print(\"Copied\", src, \"->\", dst)\n",
        "    else:\n",
        "        print(\"Not found (skipping):\", src)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWFWoowFReRp",
        "outputId": "a5a99b57-d83b-482f-f944-974130110646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied nepali_tokenizer.model -> /content/drive/MyDrive/nepali_tokenizer_models/nepali_tokenizer.model\n",
            "Copied nepali_tokenizer.vocab -> /content/drive/MyDrive/nepali_tokenizer_models/nepali_tokenizer.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "import numpy as np\n",
        "\n",
        "# Use the preprocessed file (pf)\n",
        "# Assuming 'df' with 'token' column exists from previous steps\n",
        "token_lengths = [len(tok) for tok in df['token'].tolist()]\n",
        "print(f\"\\nAverage token length: {np.mean(token_lengths):.2f} chars\")\n",
        "print(f\"Median token length: {np.median(token_lengths):.2f} chars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUfDoR5dOI6R",
        "outputId": "301c17a2-d95c-4ec4-d913-1a3649f4c33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average token length: 7.22 chars\n",
            "Median token length: 7.00 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing = []\n",
        "with open(pf, 'r', encoding='utf-8') as f:\n",
        "    for ln in f:\n",
        "        for w in ln.strip().split():\n",
        "            if \"<unk>\" in sp.encode(w, out_type=str):\n",
        "                missing.append(w)\n",
        "\n",
        "missing = list(set(missing))\n",
        "print(f\"\\nUnique words not in vocab: {len(missing)}\")\n",
        "if missing:\n",
        "    print(\"Example missing words:\", missing[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VNVxKLJOQzR",
        "outputId": "e8c87a0d-2d0e-4da9-8213-8863e50fa3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique words not in vocab: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Coverage\n",
        "import collections\n",
        "\n",
        "char_counts = collections.Counter()\n",
        "total_chars = 0\n",
        "\n",
        "with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        total_chars += len(line)\n",
        "        char_counts.update(line)\n",
        "\n",
        "# Characters in vocab\n",
        "vocab_chars = {sp.id_to_piece(i) for i in range(sp.get_piece_size()) if len(sp.id_to_piece(i)) == 1}\n",
        "\n",
        "covered_chars = sum(count for ch, count in char_counts.items() if ch in vocab_chars)\n",
        "coverage_percent = (covered_chars / total_chars) * 100\n",
        "print(f\"✅ Character Coverage: {coverage_percent:.2f}%\")\n",
        "\n",
        "# Reconstruction Accuracy\n",
        "errors = 0\n",
        "total = 0\n",
        "with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        pieces = sp.encode(line, out_type=str)\n",
        "        recon = sp.decode(pieces)\n",
        "        if recon != line:\n",
        "            errors += 1\n",
        "        total += 1\n",
        "print(f\"✅ Reconstruction Accuracy: {(1 - errors/total)*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFGBbTDRSeR_",
        "outputId": "5a7ae56d-952e-4b4d-c6a2-577bc946a36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Character Coverage: 84.49%\n",
            "✅ Reconstruction Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Load the tokenizer model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"nepali_tokenizer.model\")\n",
        "\n",
        "# Vocabulary size (should be 16000 if training succeeded)\n",
        "vocab_size = sp.GetPieceSize()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Load a sample of lines to evaluate on (e.g., first 100k lines)\n",
        "subset_path = Path(\"subset_corpus.txt\")\n",
        "\n",
        "def read_lines(file_path, max_lines=100000):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            yield line.strip()\n",
        "\n",
        "lines = list(read_lines(subset_path, max_lines=100000))\n",
        "print(f\"Loaded {len(lines)} lines for evaluation\")\n",
        "\n",
        "# Tokenize each line, calculate token lengths\n",
        "token_lengths = []\n",
        "total_tokens = 0\n",
        "oov_tokens = 0\n",
        "\n",
        "for line in lines:\n",
        "    tokens = sp.EncodeAsPieces(line)\n",
        "    token_lengths.extend([len(token) for token in tokens])\n",
        "    total_tokens += len(tokens)\n",
        "\n",
        "    # Count OOV tokens (SentencePiece uses <unk> token for unknown)\n",
        "    oov_tokens += tokens.count(sp.IdToPiece(sp.unk_id()))\n",
        "\n",
        "avg_token_length = sum(token_lengths) / len(token_lengths) if token_lengths else 0\n",
        "oov_rate = oov_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "print(f\"Average token length: {avg_token_length:.3f} characters\")\n",
        "print(f\"OOV tokens: {oov_tokens} out of {total_tokens} tokens\")\n",
        "print(f\"OOV rate: {oov_rate:.4%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QGAPYOiShvh",
        "outputId": "9c4d25d8-73f6-4120-88b7-d06675addf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 64000\n",
            "Loaded 100000 lines for evaluation\n",
            "Average token length: 5.264 characters\n",
            "OOV tokens: 0 out of 1890384 tokens\n",
            "OOV rate: 0.0000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to your saved model file\n",
        "model_file = \"/content/drive/MyDrive/nepali_tokenizer_models/nepali_tokenizer.model\"\n",
        "\n",
        "# Create a SentencePieceProcessor instance\n",
        "sp = spm.SentencePieceProcessor()\n",
        "\n",
        "# Load the model\n",
        "sp.Load(model_file)\n",
        "\n",
        "# Example text\n",
        "text = \"यो एउटा उदाहरण वाक्य हो।\"\n",
        "\n",
        "# Encode the text into pieces\n",
        "pieces = sp.encode(text, out_type=str)\n",
        "print(\"Pieces:\", pieces)\n",
        "\n",
        "# Encode the text into IDs\n",
        "ids = sp.encode(text, out_type=int)\n",
        "print(\"IDs:\", ids)\n",
        "\n",
        "# Decode pieces back to text\n",
        "decoded_text_from_pieces = sp.decode_pieces(pieces)\n",
        "print(\"Decoded from pieces:\", decoded_text_from_pieces)\n",
        "\n",
        "# Decode IDs back to text\n",
        "decoded_text_from_ids = sp.decode_ids(ids)\n",
        "print(\"Decoded from IDs:\", decoded_text_from_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6Fuz_a-UJ4l",
        "outputId": "83ecec1a-ddd1-4372-8627-f63ae208e355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pieces: ['▁यो', '▁एउटा', '▁उदाहरण', '▁वाक्य', '▁हो', '।']\n",
            "IDs: [26, 191, 1557, 9394, 17, 4]\n",
            "Decoded from pieces: यो एउटा उदाहरण वाक्य हो।\n",
            "Decoded from IDs: यो एउटा उदाहरण वाक्य हो।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gz5-pByUpru"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}